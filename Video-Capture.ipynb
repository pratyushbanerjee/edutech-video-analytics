{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from collections import OrderedDict \n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepgaze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-13d674be3afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeepgaze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepgaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepgaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_pose_estimation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCnnHeadPoseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepgaze'"
     ]
    }
   ],
   "source": [
    "import deepgaze\n",
    "from deepgaze.deepgaze.head_pose_estimation import CnnHeadPoseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e10bf37f4920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhead_pose_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnnHeadPoseEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhead_pose_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_pitch_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./deepgaze/etc/tensorflow/head_pose/pitch/cnn_cccdd_30k.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhead_pose_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_yaw_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./deepgaze/etc/tensorflow/head_pose/yaw/cnn_cccdd_30k.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhead_pose_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_roll_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./deepgaze/etc/tensorflow/head_pose/roll/cnn_cccdd_30k.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "head_pose_estimator = CnnHeadPoseEstimator(sess)\n",
    "head_pose_estimator.load_pitch_variables('./deepgaze/etc/tensorflow/head_pose/pitch/cnn_cccdd_30k.tf')\n",
    "head_pose_estimator.load_yaw_variables('./deepgaze/etc/tensorflow/head_pose/yaw/cnn_cccdd_30k.tf')\n",
    "head_pose_estimator.load_roll_variables('./deepgaze/etc/tensorflow/head_pose/roll/cnn_cccdd_30k.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left eye points: (36, 37, 38, 39, 40, 41)\n",
    "# Right eye points: (42, 43, 44, 45, 46, 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(p1 ,p2):\n",
    "    return int((p1.x + p2.x)/2), int((p1.y + p2.y)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def yaw2rotmat(yaw):\n",
    "#     x = 0.0\n",
    "#     y = 0.0\n",
    "#     z = yaw\n",
    "#     ch = np.cos(z)\n",
    "#     sh = np.sin(z)\n",
    "#     ca = np.cos(y)\n",
    "#     sa = np.sin(y)\n",
    "#     cb = np.cos(x)\n",
    "#     sb = np.sin(x)\n",
    "#     rot = np.zeros((3,3), 'float32')\n",
    "#     rot[0][0] = ch * ca\n",
    "#     rot[0][1] = sh*sb - ch*sa*cb\n",
    "#     rot[0][2] = ch*sa*sb + sh*cb\n",
    "#     rot[1][0] = sa\n",
    "#     rot[1][1] = ca * cb\n",
    "#     rot[1][2] = -ca * sb\n",
    "#     rot[2][0] = -sh * ca\n",
    "#     rot[2][1] = sh*sa*cb + ch*sb\n",
    "#     rot[2][2] = -sh*sa*sb + ch*cb\n",
    "#     return rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "# detect faces in the grayscale image\n",
    "    \n",
    "    frame = cv2.resize(frame, (560, 560))   \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #frame_number += 1\n",
    "\n",
    "    # Quit when the input video file ends\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    #rgb_frame = frame[:, :, ::-1]\n",
    "    rects = detector(gray, 1)  \n",
    "    for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "    # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        #show the face number\n",
    "        cv2.putText(frame, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        #loop over the (x, y)-coordinates for the facial landmarks\n",
    "        #and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "        landmarks = predictor(gray, rect)\n",
    "        ly_left_point = (landmarks.part(36).x, landmarks.part(36).y)\n",
    "        ly_right_point = (landmarks.part(39).x, landmarks.part(39).y)\n",
    "        \n",
    "        ly_center_top = midpoint(landmarks.part(37), landmarks.part(38))\n",
    "        ly_center_bottom = midpoint(landmarks.part(41), landmarks.part(40))\n",
    "        ly_hor_line = cv2.line(frame, ly_left_point, ly_right_point, (0, 255, 0), 2)\n",
    "        ly_ver_line = cv2.line(frame, ly_center_top, ly_center_top, (0, 255, 0), 2)\n",
    "        \n",
    "        \n",
    "        ry_left_point = (landmarks.part(42).x, landmarks.part(42).y)\n",
    "        ry_right_point = (landmarks.part(45).x, landmarks.part(45).y)\n",
    "    \n",
    "        ry_center_top = midpoint(landmarks.part(43), landmarks.part(44))\n",
    "        ry_center_bottom = midpoint(landmarks.part(46), landmarks.part(47))\n",
    "        ry_hor_line = cv2.line(frame, ry_left_point, ry_right_point, (0, 255, 0), 2)\n",
    "        ry_ver_line = cv2.line(frame, ry_center_top, ry_center_bottom, (0, 255, 0), 2)    \n",
    "    \n",
    "\n",
    "#     # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    #for face_encoding in face_encodings:\n",
    "        # See if the face is a match for the known face(s)\n",
    "#         match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)\n",
    "\n",
    "#         name = None\n",
    "#         if match[0]:\n",
    "#             name = \"Samrat\"\n",
    "    name = 'Samrat'\n",
    "    face_names.append(name)\n",
    "\n",
    "#     # Label the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left-60, top-120), (right+60, bottom+50), (0, 0, 255), 2)\n",
    "        \n",
    "        image = frame[left-60:right+60,top-120:bottom+50]\n",
    "        \n",
    "        image = cv2.resize(image, (480, 480))  \n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        #cv2.rectangle(frame, (left, bottom), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        #cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        pitch = head_pose_estimator.return_pitch(image,radians=True)\n",
    "        yaw = head_pose_estimator.return_yaw(image,radians=True)\n",
    "        roll = head_pose_estimator.return_roll(image,radians=True)\n",
    "        \n",
    "        \n",
    "#         cam_w = frame.shape[1]\n",
    "#         cam_h = frame.shape[0]\n",
    "#         c_x = cam_w / 2\n",
    "#         c_y = cam_h / 2\n",
    "#         f_x = c_x / np.tan(60/2 * np.pi / 180)\n",
    "#         f_y = f_x\n",
    "        \n",
    "#         camera_distortion = np.float32([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "#         camera_matrix = np.float32([[f_x, 0.0, c_x], [0.0, f_y, c_y], [0.0, 0.0, 1.0]])\n",
    "#         axis = np.float32([[0.0, 0.0, 0.0],[0.0, 0.0, 0.0], [0.0, 0.0, 0.5]])\n",
    "#         tvec = np.array([0.0, 0.0, 1.0], np.float) # translation vector\n",
    "        \n",
    "#         rot_matrix = yaw2rotmat(-yaw[0,0,0])\n",
    "#         rvec, jacobian = cv2.Rodrigues(rot_matrix)\n",
    "#         imgpts, jac = cv2.projectPoints(axis, rvec, tvec, camera_matrix, camera_distortion)\n",
    "        \n",
    "#         p_start = (int(c_x), int(c_y))\n",
    "#         p_stop = (int(imgpts[2][0][0]), int(imgpts[2][0][1]))\n",
    "        \n",
    "#         cv2.line(frame, p_start, p_stop, (0,0,255), 3) #Red\n",
    "#         cv2.circle(frame, p_start, 1, (0,255,0), 3) #Green\n",
    "        \n",
    "        #print(\"Estimated camera matrix: \\n\" + str(camera_matrix) + \"\\n\")\n",
    "    \n",
    "        print('data points ','pitch : ' , pitch ,' roll ' ,roll ,' yaw ',yaw)\n",
    "    \n",
    "    cv2.imshow('pose', frame)    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
